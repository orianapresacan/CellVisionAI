# U-Net++ and DeepLabV3+ segmentation models

This code is based on the [Segmentation Models Pytorch (SMP)](https://github.com/qubvel/segmentation_models.pytorch/tree/master) library.

## Prerequisites

Install all libraries from `requirements.txt`.

Download the pre-trained autoencoder from [here](https://drive.google.com/file/d/1vxMdSfCnnCA1U1R2OuBOxdAGXBxFWqCN/view?usp=sharing) and add it to the pretrained_autoencoder directory.


## Dataset preparation

The data must have the following structure:

```bash
data
│
├── train
│   ├── images
│   └── masks
├── val
│   ├── images
│   └── masks
├── test
    ├── images
    └── masks
```

Add the images and the masks to the train, test, and val directories based on the `data_division.txt` (or you can download it from [here](https://drive.google.com/file/d/1impULoCal0-gGriwiZh4ROhIlgFzwOrO/view?usp=sharing).

Next, we need to crop each image based on the mask and resize them to 64x64 pixels to ensure consistency. To do this, run `crop_images_masks.py`, which will crop all images from the `data` folder. The script will create 2 new folders: cropped_images and cropped_masks in each parent directory (train, test, val).

## Training

Run `main.py` to train the models. 2 different models are available: UNET++, DEEPLABV3+. Change `MODEL = "UNET++"` in `main.py` to choose the model you want.

## Get Segmentations

To get examples of segmented cells (real image - real mask - mask generated with the trained model) from the test data. The images will be saved in the `test_outputs` folder. 

```bash
python test.py --action=get_samples --model=UNET++ --samples=5
```

To segment all images from the test data in order to get the performance metrics afterwards, run the following command:

```bash
python test.py --action=get_segmented_cells --model=UNET++ --input_folder='data/test/cropped_images'
```

## Evaluation - Get Performance Metrics

To get the evaluation metrics such as IoU, precision, recall, F1 score, and accuracy, run the `performance_metrics.py` script. Modify the paths from _folder1_ and _folder2_ with the path to the directory containing the real masks (ground truth) and the directory with the masks generated by the trained model.

## Model Checkpoints

The model checkpoints for the DeepLabV3+ and U-Net++ trained on the CELLULAR data set can be found [here](https://drive.google.com/drive/folders/1d4dgP2NLLR83QRsSNcr5zh9OFS065QaD?usp=sharing).
